{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip\n",
    "!unzip model.zip\n",
    "!unzip util.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rts3012/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import skimage.color as sc\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from data.CustomDataset import imageDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "from model import esrt\n",
    "from EntropyL1 import EntropyL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4\n",
    "SCALE = 2\n",
    "STEP_SIZE = 200\n",
    "GAMMA = 0.5\n",
    "EPOCHS = 50\n",
    "START_EPOCH = 1\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "PATCH_SIZE = 12\n",
    "isY = True\n",
    "N_COLOURS = 3\n",
    "RGB_RANGE = 1\n",
    "TRAIN_ANNOTATIONS = 'data/metadata/train.csv'\n",
    "VALID_ANNOTATIONS = 'data/metadata/valid.csv'\n",
    "\n",
    "if(SCALE == 2):\n",
    "    DOWNSAMPLED_DIR = 'data/downSampledImages'\n",
    "else:\n",
    "    DOWNSAMPLED_DIR = 'data/downSampledImages025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([ #transforms to be used when iterating images\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = imageDataset(DOWNSAMPLED_DIR, TRAIN_ANNOTATIONS, N_COLOURS, RGB_RANGE, PATCH_SIZE, SCALE, transform)\n",
    "valid_dataset = imageDataset(DOWNSAMPLED_DIR, VALID_ANNOTATIONS, N_COLOURS, RGB_RANGE, PATCH_SIZE, SCALE, transform)\n",
    "valid_dataset.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  4320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3938386f90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "# random seed\n",
    "#seed = args.seed\n",
    "#if seed is None:\n",
    "seed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"===> Loading datasets\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "else:\n",
    "        device = \"cpu\"\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Building models\n",
      "scale 0.16666666666666666\n",
      "288\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(\"===> Building models\")\n",
    "model = esrt.ESRT(upscale = SCALE)\n",
    "l1_criterion = nn.L1Loss()\n",
    "entropyL1 = EntropyL1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Setting GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"===> Setting GPU\")\n",
    "if(device == \"cuda\"):\n",
    "    model = model.to(device)\n",
    "    #l1_criterion = l1_criterion.to(device)\n",
    "    entropyL1 = entropyL1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Setting Optimizer\n"
     ]
    }
   ],
   "source": [
    "print(\"===> Setting Optimizer\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    utils.adjust_learning_rate(optimizer, epoch, STEP_SIZE, LEARNING_RATE, GAMMA)\n",
    "    print('epoch =', epoch, 'lr = ', optimizer.param_groups[0]['lr'])\n",
    "    for iteration, (lr_tensor, hr_tensor, _, _, entropy) in enumerate(train_loader, 1):\n",
    "\n",
    "        if device==\"cuda\":\n",
    "            lr_tensor = lr_tensor.to(device)  # ranges from [0, 1]\n",
    "            hr_tensor = hr_tensor.to(device)  # ranges from [0, 1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sr_tensor = model(lr_tensor)\n",
    "        #loss_l1 = l1_criterion(sr_tensor, hr_tensor)\n",
    "        #loss_sr = loss_l1\n",
    "        loss_l1Entropy = entropyL1(sr_tensor, hr_tensor)\n",
    "        loss_sr = loss_l1Entropy\n",
    "\n",
    "        loss_sr.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"===> Epoch[{}]({}/{}): Loss_l1: {:.5f}\".format(epoch, iteration, len(train_loader),\n",
    "                                                                  loss_l1Entropy.item()))\n",
    "        running_loss += loss_l1Entropy.item()\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(scale):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    avg_psnr, avg_ssim = 0, 0\n",
    "    for batch in valid_loader:\n",
    "        lr_tensor, hr_tensor, entropy = batch[0], batch[1], batch[4]\n",
    "        if device=='cuda':\n",
    "            lr_tensor = lr_tensor.to(device)\n",
    "            hr_tensor = hr_tensor.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pre = model(lr_tensor)\n",
    "        \n",
    "        sr_tensor = model(lr_tensor)\n",
    "        #loss_l1 = l1_criterion(sr_tensor, hr_tensor)\n",
    "        loss_l1Entropy = entropyL1(sr_tensor, hr_tensor)\n",
    "        sr_img = utils.tensor2np(pre.detach()[0])\n",
    "        gt_img = utils.tensor2np(hr_tensor.detach()[0])\n",
    "        crop_size = SCALE\n",
    "        cropped_sr_img = utils.shave(sr_img, crop_size)\n",
    "        cropped_gt_img = utils.shave(gt_img, crop_size)\n",
    "        if isY is True:\n",
    "            im_label = utils.quantize(sc.rgb2ycbcr(cropped_gt_img)[:, :, 0])\n",
    "            im_pre = utils.quantize(sc.rgb2ycbcr(cropped_sr_img)[:, :, 0])\n",
    "        else:\n",
    "            im_label = cropped_gt_img\n",
    "            im_pre = cropped_sr_img\n",
    "        # print(im_pre.shape)\n",
    "        # print(im_label.shape)\n",
    "        avg_psnr += utils.compute_psnr(im_pre, im_label)\n",
    "        avg_ssim += utils.compute_ssim(im_pre, im_label)\n",
    "        running_loss += loss_l1Entropy.item()\n",
    "    print(\"===> Valid. psnr: {:.4f}, ssim: {:.4f}\".format(avg_psnr / len(valid_loader), avg_ssim / len(valid_loader)))\n",
    "    return running_loss/len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    model_folder = \"experiment/checkpoint_ESRT_x{}/\".format(SCALE)\n",
    "    model_out_path = model_folder + \"epoch_{}.pth\".format(epoch)\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    torch.save(model.state_dict(), model_out_path)\n",
    "    print(\"===> Checkpoint saved to {}\".format(model_out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    # print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training\n",
      "Total number of parameters: 677783\n",
      "===> Valid. psnr: 11.5974, ssim: 0.2037\n",
      "epoch = 1 lr =  0.0002\n",
      "===> Epoch[1](100/200): Loss_l1: 0.14414\n",
      "===> Epoch[1](200/200): Loss_l1: 0.09799\n",
      "Epoch cost times: 0:04:15.808940\n",
      "4.3m 4.3m/3.6h\n",
      "===> Valid. psnr: 26.4901, ssim: 0.6404\n",
      "epoch = 2 lr =  0.0002\n",
      "===> Epoch[2](100/200): Loss_l1: 0.08690\n",
      "===> Epoch[2](200/200): Loss_l1: 0.08742\n",
      "Epoch cost times: 0:03:39.183929\n",
      "3.7m 7.9m/3.4h\n",
      "===> Valid. psnr: 28.6201, ssim: 0.7554\n",
      "epoch = 3 lr =  0.0002\n",
      "===> Epoch[3](100/200): Loss_l1: 0.08864\n",
      "===> Epoch[3](200/200): Loss_l1: 0.05763\n",
      "Epoch cost times: 0:03:39.811795\n",
      "3.7m 11.6m/3.3h\n",
      "===> Valid. psnr: 28.7209, ssim: 0.7785\n",
      "epoch = 4 lr =  0.0002\n",
      "===> Epoch[4](100/200): Loss_l1: 0.03330\n",
      "===> Epoch[4](200/200): Loss_l1: 0.04696\n",
      "Epoch cost times: 0:03:39.657426\n",
      "3.7m 15.2m/3.2h\n",
      "===> Valid. psnr: 30.0049, ssim: 0.8101\n",
      "epoch = 5 lr =  0.0002\n",
      "===> Epoch[5](100/200): Loss_l1: 0.06872\n",
      "===> Epoch[5](200/200): Loss_l1: 0.05054\n",
      "Epoch cost times: 0:03:37.004356\n",
      "3.6m 18.9m/3.2h\n",
      "===> Valid. psnr: 30.8760, ssim: 0.8459\n",
      "epoch = 6 lr =  0.0002\n",
      "===> Epoch[6](100/200): Loss_l1: 0.06514\n",
      "===> Epoch[6](200/200): Loss_l1: 0.04612\n",
      "Epoch cost times: 0:03:37.828724\n",
      "3.6m 22.5m/3.2h\n",
      "===> Valid. psnr: 29.9880, ssim: 0.8540\n",
      "epoch = 7 lr =  0.0002\n",
      "===> Epoch[7](100/200): Loss_l1: 0.03717\n",
      "===> Epoch[7](200/200): Loss_l1: 0.04449\n",
      "Epoch cost times: 0:03:36.707491\n",
      "3.6m 26.1m/3.2h\n",
      "===> Valid. psnr: 30.6044, ssim: 0.8720\n",
      "epoch = 8 lr =  0.0002\n",
      "===> Epoch[8](100/200): Loss_l1: 0.05254\n",
      "===> Epoch[8](200/200): Loss_l1: 0.05014\n",
      "Epoch cost times: 0:03:29.403604\n",
      "3.5m 29.6m/3.1h\n",
      "===> Valid. psnr: 30.8449, ssim: 0.8720\n",
      "epoch = 9 lr =  0.0002\n",
      "===> Epoch[9](100/200): Loss_l1: 0.03840\n",
      "===> Epoch[9](200/200): Loss_l1: 0.04308\n",
      "Epoch cost times: 0:03:29.461098\n",
      "3.5m 33.1m/3.1h\n",
      "===> Valid. psnr: 30.7621, ssim: 0.9012\n",
      "epoch = 10 lr =  0.0002\n",
      "===> Epoch[10](100/200): Loss_l1: 0.05829\n",
      "===> Epoch[10](200/200): Loss_l1: 0.04397\n",
      "===> Checkpoint saved to experiment/checkpoint_ESRT_x2/epoch_10.pth\n",
      "Epoch cost times: 0:05:01.144006\n",
      "5.0m 38.1m/3.2h\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m t_epoch_start \u001b[38;5;241m=\u001b[39m timer\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m      9\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 10\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m valid(SCALE)\n\u001b[1;32m     11\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(valid_loss)\n\u001b[1;32m     12\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train(epoch)\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(scale)\u001b[0m\n\u001b[1;32m      9\u001b[0m     hr_tensor \u001b[38;5;241m=\u001b[39m hr_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     pre \u001b[38;5;241m=\u001b[39m model(lr_tensor)\n\u001b[1;32m     14\u001b[0m sr_tensor \u001b[38;5;241m=\u001b[39m model(lr_tensor)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#loss_l1 = l1_criterion(sr_tensor, hr_tensor)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/MastersProject/InitialNotebooks/model/esrt.py:211\u001b[0m, in \u001b[0;36mESRT.forward\u001b[0;34m(self, x1, x2, test)\u001b[0m\n\u001b[1;32m    209\u001b[0m body_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_blocks):\n\u001b[0;32m--> 211\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody[i](x1)\n\u001b[1;32m    212\u001b[0m     body_out\u001b[38;5;241m.\u001b[39mappend(x1)\n\u001b[1;32m    213\u001b[0m res1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(body_out,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/MastersProject/InitialNotebooks/model/esrt.py:152\u001b[0m, in \u001b[0;36mUn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# out = self.encoder3(self.encoder2(self.encoder1(x)))\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder1(x)\n\u001b[0;32m--> 152\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder2(x1)\n\u001b[1;32m    153\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder3(x2)\n\u001b[1;32m    154\u001b[0m     out \u001b[38;5;241m=\u001b[39m x3\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/MastersProject/InitialNotebooks/model/esrt.py:126\u001b[0m, in \u001b[0;36mUpdownblock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 126\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m    127\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown(x1)\n\u001b[1;32m    128\u001b[0m     high \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x2, size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/MastersProject/InitialNotebooks/model/esrt.py:106\u001b[0m, in \u001b[0;36mone_module.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    105\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 106\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x1)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# x3 = self.layer3(x2)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malise(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight2(x2),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight3(x1)],\u001b[38;5;241m1\u001b[39m))))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/MastersProject/InitialNotebooks/model/esrt.py:58\u001b[0m, in \u001b[0;36mone_conv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight1(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight1(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"===> Training\")\n",
    "print_network(model)\n",
    "code_start = datetime.datetime.now()\n",
    "timer = utils.Timer()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(START_EPOCH, EPOCHS + 1):\n",
    "    t_epoch_start = timer.t()\n",
    "    epoch_start = datetime.datetime.now()\n",
    "    valid_loss = valid(SCALE)\n",
    "    val_losses.append(valid_loss)\n",
    "    train_loss = train(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    if epoch%10==0:\n",
    "        save_checkpoint(epoch)\n",
    "    epoch_end = datetime.datetime.now()\n",
    "    print('Epoch cost times: %s' % str(epoch_end-epoch_start))\n",
    "    t = timer.t()\n",
    "    prog = (epoch-START_EPOCH+1)/(EPOCHS + 1 - START_EPOCH+ 1)\n",
    "    t_epoch = utils.time_text(t - t_epoch_start)\n",
    "    t_elapsed, t_all = utils.time_text(t), utils.time_text(t / prog)\n",
    "    print('{} {}/{}'.format(t_epoch, t_elapsed, t_all))\n",
    "code_end = datetime.datetime.now()\n",
    "print('Code cost times: %s' % str(code_end-code_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "model_folder = \"experiment/checkpoint_ESRT_x{}/\".format(SCALE)\n",
    "model_out_path = model_folder + \"epoch_{}.pth\".format(epoch)\n",
    "files.download(model_out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
